\documentclass[main]{subfiles} 
\graphicspath{{img/}}


\begin{document}

\section{Implementation}
\begin{itemize}
    \item Scrapy -  version 2.6.1
    \item Selenium - version 4.1.5
    \item Webdriver\_manager - version 3.5.4
    \item Numpy -  version 1.22.3
    \item Pandas  - version 1.4.2
    \item Time
    \item Datetime
    \item Openpyxl - version 3.0.9
    \item Tk (tkinter) - version 0.1.0
    \item Pillow - version 9.1.1
\end{itemize}

\subsection{Structure of the project}

\subsection{Implementation of the webscrapping}

\subsection{Implementation of the GUI}

\subsubsection{Organizing the scrapped data}
The first step to building the GUI is organizing the data that we obtain from the webscrapping.
We first analyse the dataset and then clean it for the GUI to function correctly.
This is done in the program cleaning_database.py which we also use to do the data analysis in our parallel project. \par
In this cleaning program, we first load the dataset. 
Then we print the headings and the description to check the data and the types.
We remove variables that are mispelled, uncorrect or missing. 
We thenproceed to split the address from the zipcode, to get the zip code in a separate column. 
We do this as we want to be able to search properties within a certain zipcode. 
Finally we save the dataset. This dataset is the one that the GUI will use to return information to the user.

\subsubsection{Building the GUI}

\subsection{Maps?}

\end{document}